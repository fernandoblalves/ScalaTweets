#overwriting or not results; useful for stoping and resuming tests
overwriting=true
#handling features
num_features=3000
#split ratio is for spliting the train dataset into train and test; the presented value is the train share
split_ratio=0.6
#feature extraction methods are TF-IDF or W2V
feature_extractor=W2V
#implemented classifiers are SVM or MLP
classifier=SVM
#MLP hidden layers size
mlp_hidden_layers=7
mlp_hidden_perceptrons=14
mlp_iterations=200
#SVM params
svm_c=2.0
svm_stepsize=0.1
svm_iterations=100
#mode is "train" to create and save a model, "train_and_test" uses the dataset for training the model and testing it,
# "classify" using existing an model, or "cross_validate" to search for optimal parameters
mode=w2v_test
TF-IDF_dataset=/home/fernando/Documents/DiSIEM/tweets/early_disiem_dataset/d1/A
train_dataset_path=/home/fernando/Documents/DiSIEM/tweets/fixed_500
validate_dataset_path=/home/fernando/Documents/DiSIEM/tweets/early_disiem_dataset/d2/A
model_name_suffix=wat
#general data
cross_validation_folds=10
stopwords_file=/home/fernando/Documents/DiSIEM/spark/data/stopwords/stopwords
save_model_path=/home/fernando/Documents/DiSIEM/spark/data/results/
keyword_path=""
w2v_vectors_path=/home/fernando/Documents/DiSIEM/w2v_py/w2v.model
w2v_vector_size=300
#clustering
#kmeans or DBSCAN
clusterer=kmeans
#kmeans
num_iterations=50
#DBSCAN
eps=7
pca=30
distance_measure=euclidean
#reclustering
column_threshold=0
table_threshold=50